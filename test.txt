Paragraph 1:
Text: Hi, this is Eric Johnson. It's February 18, 2021, and this is the engineering key review at GitLab. So I've got number four in the agenda, which is a proposal to break up this meeting into four department key reviews. So currently this is engineering. Development quality, security, and UX infrastructure and support to their own key reviews already. I have the reasons why increased visibility able to go deeper. Increased the objectivity with which my reports can manage their groups, allow me more time to focus on new markets and allow me to shift into more of a question asker mode, then generating content and answering questions in these meetings. And to avoid adding three net new meetings to stakeholders counters, I propose we do a sort of two month rotation. So month one development quality go month two security and UX would go. How do people feel about that proposal? I think in the group conversations it's working really well. So I'm supportive and this is the smallest thing. Maybe we need four meetings a month, like it's the biggest department, it's super central. But you proposed this. I don't. I could see either way. So let's stick with the proposal. Go, we'll try it and we'll we can be flexible. I mean, development is larger. Maybe they go more frequently or something. I'll see how it goes.
Key Points:
- Eric Johnson opened the GitLab Engineering Key Review on February 18, 2021.
- A proposal was made to break the current engineering key review into four department-specific key reviews (Development, Quality, Security, UX).
- Reasons for the split include increased visibility, deeper dives, greater objectivity for managers, more strategic focus for Eric, and a shift in Eric's role to a question-asker.
- A two-month rotation schedule was proposed to avoid adding too many new meetings: Month 1 (Development, Quality), Month 2 (Security, UX).
- The proposal was generally supported, with flexibility for larger departments like Development to potentially meet more frequently.

Paragraph 2:
Text: All right. And then I've got number five, which is we've got R&D overall MR rate. And we also have R&D wider MR rate, both as top level KPIs for engineering. So the difference between them in the simplest sense is that R&D wider MR rate includes both community contributions and community MRs. The problems I see with this are that one, the wider MR rate, the one that includes internally and external MRs, it duplicates the overall MR rate, which is. The wider MR rate should just be external, right? And then overall should be narrow plus wider. Oh, yeah. Like we say the wider community. Right, right, right. Okay. So there's. I have to check the taxonomy. Well, like, can you can you confirm? That's that's, that's, that's, it's reasoning is my understanding as well. Yeah, I believe water MR rate just captures community contributions. Only and no internal. Yeah, and the reason we measure data is that like one of the most likely failure modes is that we lose the community.
Key Points:
- The discussion moved to R&D overall MR rate and R&D wider MR rate, both top-level engineering KPIs.
- Initial confusion arose regarding the definition of "wider MR rate," with Eric suggesting it duplicated the overall rate and should be external-only.
- Lily clarified that "wider MR rate" specifically captures *only* community contributions and no internal ones.
- The purpose of measuring wider MR rate is to monitor and prevent the loss of community engagement, which is considered a likely failure mode.

Paragraph 3:
Text: Yeah. So we're, we're, we're gets goofy is the, when you look at a specific team within the company, there could be contributions outside of that that aren't community contributions. They would be viewed community contributions by that group, but effectively, they're not from outside the company. So that's why we use wider to kind of reflect that and narrow is very specific to the team. Are you saying that if someone in plan contributes to verify its viewed as wider? Not quite that plan plan and verify are just fine. It's when you look at like the development versus infrastructure infrastructure will oftentimes contribute to developments work, but it won't be counted as MR's. Yeah, that's a, that's a potential bit of funkiness that we should talk about separately. I didn't have that in my sort of critique of this, but that that doesn't necessarily make intuitive sense to me. So, so then I think part of my critique of this can be thrown out because it's not as duplicative as I thought, but I still think there's a problem with R&D wider MR rate, which is this thing doesn't really move. In part because it's a rate, so it feels like the way to drive this up is to specifically drive community authors to contribute more than one MR per month. That's how this moves up because it's a productivity rate like we use internally. And that doesn't necessarily feel like the right thing because there's scenarios in which this goes up. We've actually got less contributions overall and less contributors overall. Wait, wait, wait a second. So you're saying that R&D wider MR rate is number MR's per external contributor. Oh my goodness, that should not be the thing. It should be contributions for GitLab team member. So to contribute those, the thing above the division is the external ones. The thing below is the number of team members at GitLab. Is that the case, Lily? I'm checking right now. I think so in our MR's per team member. So unless we start calling people outside the company team member, then it shouldn't be that. Yeah, just clarifying here. So our numerator is community contributions and then our denominator is GitLab team members. So it's not per external member. So we've got what we're doing there, Eric, is we're not trying to say how many MR's does someone send if they send something? How many MR's from external do we get for the size of our organization? Sorry, I have a childhood emergency outside the door. So maybe explain the context behind this. The context is as we grow as a company, we should make sure we keep the community up. Like the logical thing is for the community to flatline and the size of the org to go and before you know, you've kind of outgrown the wider community.
Key Points:
- "Wider" MRs also capture internal contributions across different departments (e.g., infrastructure contributing to development) that aren't external community contributions but aren't "narrow" to the team. This "funkiness" needs separate discussion.
- Eric's revised critique of the R&D wider MR rate: it rarely moves, and as a rate (MRs per external contributor), it could incentivize driving more MRs from existing contributors, potentially reducing overall contributions or contributors.
- Lily clarified the R&D wider MR rate: the numerator is community contributions, and the denominator is GitLab team members, meaning it measures external MRs per the size of the internal organization, not per external member.
- The metric's original context is to ensure community contributions scale with the company's growth, preventing the community from being "outgrown."

Paragraph 4:
Text: Yeah, what I'm seeing is we created this pre-sophisticated taxonomy with prefixes and postfixes to talk about these things. But in reality, we've only got two of them. And it's we keep forgetting, we have a hard time discussing this thing. So I'd rather just name them simply two names for what they are rather than using the taxonomy. But also like in F, I have this proposal of like, what if we just track as a KPI of the percentage of total MR's that come from the community over time? And we would see that drop. I love that. Let's do that instead. But the thing, the thing why we have this complex thing is because you can game that you want to game that you just produce fewer MR's with the engineers that get one. So if you drive that really hard and say this is your number one goal, it's very easy to achieve is just tell all your engineers to produce half. Yeah, so we have we have different metrics to prevent that from happening the same way that like support SLAs and asset kind of buttress one another. I think we're we're robust to that. But simplifying this would make these conversations. If you as our CTO don't even understand them, we went overboard. So I'm supportive. And I forgot and I was agreeing the stuff this morning and like, there's a problem with this and then you just remind me of the context. So yeah, I can't really have my head. A percentage that come from the community. I love that. It's what all our investors ask about. Let's do that.
Key Points:
- The current complex taxonomy for MR rates makes them hard to understand and discuss.
- Eric proposed simplifying the naming of the MR rates.
- A new KPI proposal: tracking the "percentage of total MRs that come from the community over time."
- Concern was raised that this percentage metric could be gamed by engineers producing fewer internal MRs.
- This concern was countered by explaining that other metrics exist to prevent such gaming, and simplifying the metric would improve clarity, especially since even the CTO found the current ones confusing.
- The new percentage-based metric was strongly supported as it aligns with investor interests.

Paragraph 5:
Text: Okay. Cool. So, um, Lily, if you can work with Max to make that transition of the great. And I'll hold the one that we're talking about. That's from the mulberry. Thanks. I'm on the call. Sorry, it was a bit late. Timeline. We do have PIs on the raw number of community MRs and we can make the shift. And why they're confirming from the definition, I think why they only counts for community. And that's that's what the definition is. Cool. All right. So number six, then Christopher. Sorry, I was looking up to see if I had the percentage graph because I think we played around with this at one point and had a draft of that. Probably about five months back, if I can remember, Lily. Just in FYI month of February, if you were looking at any particular metrics, particularly in development at MR rate. We haven't had updates in four days. There's apparently a lag issue that's been problematic for the data team to basically get updated metrics and they're working on that.
Key Points:
- Lily and Max are tasked with transitioning to the new percentage-based community MR metric.
- Lily confirmed that "wider" MRs, by definition, count only community contributions.
- Christopher noted a data lag issue in February, with development MR rate metrics not updating for four days, impacting the data team.

Paragraph 6:
Text: Okay. I'm sorry, Max. You get the next one. So yeah, there's some some color there on the on the medication, the lag later on. On to on to seven. We continue. I said FYI in addition to the TBI status. I'm sorry. I wanted to just touch on the postgres replication issue there real quick. I've been trying to get my arms wrapped around it. Do we have the right attention to this? This is kind of Eric. I don't know if you were commenting on hinting towards this in the last meeting around some of the infrastructure improvements on the product side. I'm just not quite sure whose responsibility it is to focus on getting a handle on some of the constraints we have on replication. So what I was mentioning in the in the product here view about an hour ago is I think is sort of like unrelated. And so I think the DRI needs to be your kind of data engineering team. But of course there's a dependency on infrastructure because that's where the data is being piqued from. They they do on that data source. So let's say for the replication lag on that slave host where I'm sorry not the on the the secondary host where the data is being pulled from like infrastructure would be the DRI for that. And so any escalations but well all those and I know we have an action plan for that as far as creating another dedicated host just for the data team to pull from. I did ask I saw that issue and I did talk to Craig gums a little bit as well on the database side just to see if there's some database improvements and I'm still trying to figure out you know if it's truly just dedicated computation old sort of resource a server or if there's actually some some database tuning that needs to occur. Do you have a sense of that? There's so I'd say it's it's three different things. It's having a dedicated host that doesn't have conflicting query traffic coming from other other workloads. There are some tuning performance or tuning improvements to be made and then there's also improvements in and this is where it does maybe relate a little bit to what the topic was in the last review basically the overall demand on the database layer from from dot com activities and like improving though. It's definitely not just one of those things that one of the most specific actions we're going to take though is separating out and having a dedicated host so that we're just dealing with the profile of the data engineering traffic on there and not having conflicting query. conflicting queries affect the ability to update the replication. Steve I definitely want to partner with you on this one because I think the the demand on those databases is only going to increase it's not going down. And I think we need to get I'm still unclear on where to focus and to get the biggest bang for the buck. I think of the computational resource dedication that's going to be a good thing but I'll probably going to squeeze the balloon and then the next area will will honor itself. Okay, I'll put into the infrick you review for next week.
Key Points:
- The discussion moved to a PostgreSQL replication issue, with concerns about attention and responsibility.
- The DRI (Directly Responsible Individual) for the replication issue is the data engineering team, but infrastructure is responsible for the secondary host from which data is pulled.
- An action plan is in place to create a dedicated host specifically for the data team to pull data from.
- The problem is multifaceted, involving three things: a dedicated host to avoid conflicting query traffic, database tuning improvements, and overall demand on the database layer from dot.com activities.
- The immediate specific action is to separate and create a dedicated host to manage data engineering traffic without conflicts.
- Steve wants to partner on this issue, as database demand is expected to increase, and he plans to add it to the infrastructure key review next week.

Paragraph 7:
Text: Okay, on this issue. Thank you, Steve. So then back to mecon 7 or yes. Thank you. And Rob was in these assassins that this morning as well. Brian, we have the attention there. Number seven, just provide provide an update on previous conversations. We continue to improve defect tracking and against SLOs. There is a first iteration P.I. that we are experimenting to show percentage of defects meeting the SLOs key findings as ones are hovering at 80% as to at 60. We've been focused mostly on S1 as to this point as hence why S3 and S4 are a lower. This will likely be the case. We are also in point B are working on the measurement for average open box age. This would give us a whole picture of what's left. If the age goes up or down to be you're cleaning the backlog the average age should go down as well. There's no P.I yet but I just want to update and ensure beyond this. It's not a off track. Number C Craig on S2. I just was looking through the charts and I noted that there was a spike in meantime to close. I just wanted to see if you had any insight into that for us. This is the S2. The S1 looked fine. This is where the point B on age and supplement those charts in the back end helps. I haven't seen a dip in age nor the count overall. I think it's the latter. We need to dig in a bit deeper in that and also the data lag. I would like to leave out when we have a whole picture when everything is synced in as well. Christie you have some insights. Yeah I'm just wondering if part of this could be the fact that we changed the severity across the board for MRs to S2. We may have some older bugs in there that hadn't been addressed because they were at a lower severity. Now we've moved them to S2 and maybe that caused a little spike. That could be the case that we did in a limited fashion. It won't be a huge volume. We also iterated after that to P9 priority since product owns prioritization. So I wouldn't account it entirely to that. I mean this isn't the infer key review but I know that they've gotten backed up on those issues. So if some some good portion of those are infer created or related then that might be lifting it as well. I can take the deeper dig in and then provide an update next time. I think we need extra debug slicing of the data here.
Key Points:
- An update was provided on defect tracking against SLOs (Service Level Objectives).
- A first iteration PI is being experimented with to show the percentage of defects meeting SLOs: S1 defects are at 80% achievement, S2 at 60%, with S3/S4 being lower due to focus on S1/S2.
- Work is underway to measure "average open bug age" to provide a holistic view of the backlog, with the expectation that average age should decrease as the backlog is cleaned.
- Craig noted a spike in S2 meantime to close and asked for insights.
- Christie suggested the spike might be due to changing the severity of older, previously low-priority MRs to S2.
- This explanation was partially discounted, noting it was limited, and priority was later shifted to P9.
- It was suggested that infrastructure-related issues, where they are backed up, might also be contributing to the spike.
- Christopher will conduct a deeper investigation with extra data slicing and provide an update at the next meeting.

Paragraph 8:
Text: So do you like to go to point eight? Yeah we are now measuring S1 as to SLO achievement with closed bugs. But if you then look at the number of bugs it's exponential growth. And then it will be trivial to achieve 100% SLO achievement. If you just look at closed bugs even though there would be a major problem in the company 99% of all bugs are overdue. As long as I only close ones that are still within the SLO I'll have great achievement. So I think we shouldn't be looking at the closed bugs. I think we should be looking at open bugs the entire population or percentage of doses within the SLO time. I think we're doing it the wrong way. Thanks for the feedbacks that hence why we wanted to have the average age to measure what's outside in the open. We can make this iteration to measure also measure focus on the age of all opened or including open bugs. This is also something we I have discussed with Christopher in the next iteration as well. And we happy we're happy to adjust. So the key. So average age would get closer to it. It's not what I'm proposing when I'm proposing is off the open books are percentages outside of SLO. So display it as a percentage you do now just do it about the open bugs not the close. Got it. Okay. The exceeding SLO for open box. Yeah, or open box that are within SLO. So you have a chart that should go up and to the right like everything else. Sounds great. We can take it to the next data metrics work stream to deliver this. Cool. Thanks. Yeah. You'll have to figure out how to. Because we like to be able to have charts that we can historically reconstruct if we need to. So in tickets close out you need to go through their history to figure out at best time when it was open. Did it breach the SLO or not? That's a good point. This might be much harder computationally. And so I totally respect if we can do it for that reason.
Key Points:
- Sid critiqued the current S1/S2 SLO achievement measurement, which focuses on *closed* bugs, stating it can be easily gamed (e.g., closing only bugs within SLO while many open ones are overdue).
- He proposed shifting the measurement to *open* bugs, specifically the percentage of open bugs that are within or outside their SLO.
- The team acknowledged the feedback, noting "average open bug age" was intended to address this, and agreed to adjust to measure the age of all open bugs.
- Sid clarified further: the metric should be a percentage of open bugs that are either within or exceeding their SLO, aiming for a "go up and to the right" chart.
- The team agreed to take this to the next data metrics workstream for implementation.
- A potential computational challenge was noted: historically reconstructing whether a closed ticket breached SLO requires reviewing its full history.

Paragraph 9:
Text: Cool. Nine Craig. Yeah, I just wanted to ask the team like I went through all the key meeting metrics. Everything looked in line with prior periods and look good. Is there anything the team wants to call out? Yeah, I'll call out. So the good news is in Q four. We had our smallest decline over several quarters. So we only went down by a tenth of a point. The quarter previous was point six or six tenths of a point and the quarter before that was a full point. So we see this as an improvement, even though it was still a decline, but it's still a decline. Obviously we want this actually tracking in an upward direction. We also don't have enough data to know whether or not this is an actual real trend up. So I'm optimistic. I think this is a good thing. We have had a much keener focus on us over the past several quarters. So that's why I think, okay, the work that we've done, I think actually is catching up and getting noticed in sus, but we got to keep an eye on it. We can't we cannot assume that that's the case. Yeah, and the bug discussion above just kind of points out that like we have an underlying problem right now in our metrics measurement. So if we change the measurements to reflect that, then hopefully we're in good shape. If we don't and we flat line and address it so that we flat line open S once in S twos, you will see a temporary jump in and above SLOs. As we clear out that back, I'll go over that period of time. So we have a point C, which is similar to infrastructure, we need to get more security work prioritized or hearing that from the team, but neither that problem or that activity is sort of currently reflected in our security metrics. So we have some work to do long term to make sure the we see things like that in the metrics and the measurements that we're making.
Key Points:
- Craig asked the team if there were any key meeting metrics to highlight, as most appeared in line with prior periods.
- A positive highlight: The System Usability Scale (SUS) saw its smallest decline (0.1 points) in Q4 over several quarters, showing an improvement trend, though it's still a decline and not yet a confirmed upward trend.
- This improvement is attributed to a stronger focus on SUS over recent quarters.
- The previous bug discussion indicates an underlying problem in metrics measurement, suggesting changes are needed to reflect true status.
- A challenge similar to infrastructure was noted: more security work needs prioritization, but this activity isn't currently reflected in existing security metrics.
- Long-term work is required to ensure security activities are adequately captured in metrics.

Paragraph 10:
Text: So back to you said 10. So now, now, MR rate seems significantly below target. And maybe I helped that it would bounce back from December, I think it bounced back, but not back on target any any context there what's going on. Yeah, so with family and friends days, we actually had some heavier of vacation days. And January, then we historically have one thing to know is that we are actually at a higher MR rate. If you look, if you go back the last 18 months, we're actually at a higher near MR rate than we were back in each month this year. So if you compare October, October, November to November and January, comparatively to last year, which will find us were between a half point and a 1.5 MR rate above where we were in the month of previous year. That's great contact. Thank you, Chris, for good work. Yeah, so the expectation is is that February is a short month. We are at I think 16 work days with friends and family day and other things. Obviously seven carriages in Texas doesn't help things either for the folks who are working in Texas. But hopefully the rest of the team is being effective. I was hoping to see a better result right now, but with four or five days, particularly around release week. That's usually when we see do do see a little bit higher activity. So that's not a kind of for yet. But you know, Marches, Marches when I'm expecting kind of see a real rebound much like we did last year. Awesome. Thanks. The other context I'll give is we now do time series targets. So when we change the target, you'll see that reflected in the line. So if we were to look back historically here, the goal here was actually lower and Christopher was ambitious and we kept raising it, we kept meeting that. So it should stare step here and we could go back and reconstruct that if we really wanted to. And then ignoring the sort of the seasonal dip here, we raised it to I think 11 and then we realized we're kind of hitting that point of doing and raising returns and the right thing to do business wise. And this is in our FY 22 direction is hold the line at productivity. But start to raise other things related to quality security availability and what not. So that's kind of what you are you seeing this bump is we raised it and we realized, okay, that's not the we shouldn't raise it anymore and we brought it back down to 10. So 10 will be the dark going forward. I'm going to try to get better a lot of other things while preventing this from dipping. And just want to call out that it's not like higher MR rates should also help to address security and quality and other things because you're more productivity can fix more things. So it's not necessarily opposite. But I agree with that. Let's hold the line tennis and is a great number and focus on other indicators to improve that makes it on a sense. So we'll say. All right, that's the agenda and you want to want to vocalize anything else. Great, well, thanks everybody and I'm going to go check on my four year old and see if she got what whatever she needed. So cheers and toxic. Thanks, sir.
Key Points:
- Near MR rate is currently below target, despite a slight bounce back from December.
- Context for lower January performance: heavier vacation days due to family and friends days.
- Despite being below target, the current near MR rate is historically higher (0.5 to 1.5 points) compared to the same months last year.
- Expectations for February: a short month (16 workdays) with additional challenges like family/friends day and Texas weather; a rebound is anticipated in March, similar to last year.
- The company uses time series targets for MR rate; the target was previously raised to 11 due to ambitious goals and then brought back down to 10.
- The FY22 direction is to hold the productivity line at 10 MRs per team member and focus on improving other areas like quality, security, and availability.
- It was noted that higher MR rates (productivity) can also indirectly help address quality and security issues.

Tasks:
- Task 1:
  Assigned to: Eric Johnson (implicitly, as proposer)
  Task: Oversee the implementation and trial of the new departmental key review structure.
  Details: Implement a two-month rotation (Month 1: Development, Quality; Month 2: Security, UX). Be flexible, especially for larger departments like Development, which might need more frequent reviews.
- Task 2:
  Assigned to: Lily and Max
  Task: Transition the community MR rate metric.
  Details: Work together to shift from the current "R&D wider MR rate" (community contributions per GitLab team member) to a new KPI tracking the "percentage of total MRs that come from the community over time."
- Task 3:
  Assigned to: Data engineering team (DRI) with dependency on Infrastructure.
  Task: Address the PostgreSQL replication issue.
  Details: Ensure proper attention to the issue, focusing on dedicated computation resources, potential database tuning, and managing overall demand on the database layer. An action plan includes creating a dedicated host for the data team to pull from.
- Task 4:
  Assigned to: Christopher
  Task: Conduct a deeper investigation into the spike in S2 meantime to close.
  Details: Perform "extra debug slicing of the data" to understand the causes behind the spike and provide an update at the next meeting.
- Task 5:
  Assigned to: Data metrics workstream
  Task: Deliver a new defect SLO measurement.
  Details: Implement a new metric that measures the percentage of *open* bugs that are within or exceeding their SLO, rather than focusing on closed bugs. Consider computational challenges for historical reconstruction.
- Task 6:
  Assigned to: Steve
  Task: Add the PostgreSQL replication issue to the infrastructure key review agenda.
  Details: Schedule a discussion for next week's infrastructure key review to partner on addressing the increasing demand on databases.
- Task 7:
  Assigned to: (Implicitly the Engineering/Security team)
  Task: Prioritize more security work and ensure it's reflected in metrics.
  Details: Long-term work is needed to create metrics that accurately capture the prioritization and progress of security-related activities, as current metrics don't reflect this.